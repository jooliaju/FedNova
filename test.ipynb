{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5c3612a",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6428e8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "import math\n",
    "from math import ceil\n",
    "from random import Random\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.utils.data.distributed\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.multiprocessing import Process\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.models as models\n",
    "\n",
    "from distoptim import FedProx, FedNova\n",
    "import util_v4 as util\n",
    "import logging\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5821ace7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NIID': False,\n",
       " 'alpha': 0.2,\n",
       " 'backend': 'nccl',\n",
       " 'bs': 32,\n",
       " 'datapath': './data/',\n",
       " 'gmf': 0,\n",
       " 'localE': 1,\n",
       " 'lr': 0.1,\n",
       " 'model': 'VGG',\n",
       " 'momentum': 0.0,\n",
       " 'mu': 0,\n",
       " 'name': 'default',\n",
       " 'optimizer': 'fednova',\n",
       " 'p': False,\n",
       " 'pattern': 'constant',\n",
       " 'print_freq': 100,\n",
       " 'rank': 0,\n",
       " 'rounds': 4,\n",
       " 'save': True,\n",
       " 'savepath': './results/',\n",
       " 'seed': 1,\n",
       " 'size': 1}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser(description='CIFAR-10 baseline')\n",
    "parser.add_argument('--name','-n', \n",
    "                    default=\"default\", \n",
    "                    type=str, \n",
    "                    help='experiment name, used for saving results')\n",
    "parser.add_argument('--backend',\n",
    "                    default=\"nccl\",\n",
    "                    type=str,\n",
    "                    help='background name')\n",
    "parser.add_argument('--model', \n",
    "                    default=\"VGG\", #default: res\n",
    "                    type=str, \n",
    "                    help='neural network model')\n",
    "parser.add_argument('--alpha', \n",
    "                    default=0.2, \n",
    "                    type=float, \n",
    "                    help='control the non-iidness of dataset')\n",
    "parser.add_argument('--gmf', \n",
    "                    default=0, \n",
    "                    type=float, \n",
    "                    help='global (server) momentum factor')\n",
    "parser.add_argument('--lr', \n",
    "                    default=0.1, \n",
    "                    type=float, \n",
    "                    help='client learning rate')\n",
    "parser.add_argument('--momentum', \n",
    "                    default=0.0, \n",
    "                    type=float, \n",
    "                    help='local (client) momentum factor')\n",
    "parser.add_argument('--bs', \n",
    "                    default=32, \n",
    "                    type=int, \n",
    "                    help='batch size on each worker/client')\n",
    "parser.add_argument('--rounds', \n",
    "                    default=4, \n",
    "                    type=int, \n",
    "                    help='total coommunication rounds')\n",
    "parser.add_argument('--localE', \n",
    "                    default=1, \n",
    "                    type=int, \n",
    "                    help='number of local epochs')\n",
    "parser.add_argument('--print_freq', \n",
    "                    default=100, \n",
    "                    type=int, \n",
    "                    help='print info frequency')\n",
    "parser.add_argument('--size', \n",
    "                    default=1, \n",
    "                    type=int, \n",
    "                    help='number of local workers')\n",
    "parser.add_argument('--rank', \n",
    "                    default=0, \n",
    "                    type=int, \n",
    "                    help='the rank of worker')\n",
    "parser.add_argument('--seed', \n",
    "                    default=1, \n",
    "                    type=int, \n",
    "                    help='random seed')\n",
    "parser.add_argument('--save', '-s', \n",
    "                    default=True, \n",
    "                    action='store_true', \n",
    "                    help='whether save the training results')\n",
    "parser.add_argument('--p', '-p', \n",
    "                    action='store_true', \n",
    "                    help='whether the dataset is partitioned or not')\n",
    "parser.add_argument('--NIID',\n",
    "                    \n",
    "                    action='store_true',\n",
    "                    help='whether the dataset is non-iid or not')\n",
    "parser.add_argument('--pattern',\n",
    "                    default='constant',\n",
    "                    type=str, \n",
    "                    help='pattern of local steps')\n",
    "parser.add_argument('--optimizer', \n",
    "                    default='fednova',  ##default is 'local'\n",
    "                    type=str, \n",
    "                    help='optimizer name')\n",
    "# parser.add_argument('--initmethod',\n",
    "#                     default='tcp://h0:22000',\n",
    "#                     type=str,\n",
    "#                     help='init method')\n",
    "parser.add_argument('--mu', \n",
    "                    default=0, \n",
    "                    type=float, \n",
    "                    help='mu parameter in fedprox')\n",
    "parser.add_argument('--savepath',\n",
    "                    default='./results/',\n",
    "                    type=str,\n",
    "                    help='directory to save exp results')\n",
    "parser.add_argument('--datapath',\n",
    "                    default='./data/',\n",
    "                    type=str,\n",
    "                    help='directory to load data')\n",
    "\n",
    "\n",
    "logging.basicConfig(format='%(levelname)s - %(message)s', level=logging.INFO)\n",
    "logging.debug('This message should appear on the console')\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "arg_defaults = {}\n",
    "for key in vars(args):\n",
    "    arg_defaults[key] = parser.get_default(key)\n",
    "arg_defaults\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4eb567a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class client:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.optimizer = None\n",
    "        self.model=None\n",
    "        self.criterion = None\n",
    "    \n",
    "    algorithms = {\n",
    "            'fedavg': FedProx, # mu = 0\n",
    "            'fedprox': FedProx,\n",
    "            # 'scaffold': Scaffold,\n",
    "            'fednova': FedNova,\n",
    "            # 'fednova_vr':FedNovaVR,\n",
    "        }\n",
    "        \n",
    "    def run(self, init_run:bool, rank: int, size: int, round:int, opt:int):\n",
    "        # initiate experiments folder\n",
    "        save_path = args.savepath\n",
    "        folder_name = save_path+args.name\n",
    "\n",
    "        if rank == 0 and os.path.isdir(folder_name)==False and args.save: #first client\n",
    "            os.makedirs(folder_name)\n",
    "        # dist.barrier()\n",
    "\n",
    "        # initiate log files\n",
    "        tag = '{}/lr{:.3f}_bs{:d}_cp{:d}_a{:.2f}_e{}_r{}_n{}.csv'\n",
    "        saveFileName = tag.format(folder_name, args.lr, args.bs, args.localE, \n",
    "                                args.alpha, args.seed, round, size)\n",
    "        args.out_fname = saveFileName\n",
    "        with open(args.out_fname, 'w+') as f: #opens this file in results/default and writes the following print statement to it\n",
    "            print(\n",
    "                'TRAINING\\n'\n",
    "                'World-Size,{ws}\\n'\n",
    "                'Batch-Size,{bs}\\n'\n",
    "                'Round {rnd}'\n",
    "                'Epoch,itr,'\n",
    "                'Loss,avg:Loss,Prec@1,avg:Prec@1,val,time'.format(\n",
    "                    ws=args.size,\n",
    "                    bs=args.bs, rnd=round),\n",
    "                file=f)\n",
    "    \n",
    "\n",
    "        # seed for reproducibility\n",
    "        torch.manual_seed(args.seed)\n",
    "            \n",
    "        if init_run:\n",
    "            # load datasets\n",
    "            self.train_loader, self.test_loader, self.DataRatios = \\\n",
    "                util.partition_dataset(rank, size, args)\n",
    "            logging.debug(\"Worker id {} local sample ratio {} \"\n",
    "                        \"local epoch length {}\"\n",
    "                        .format(rank, self.DataRatios[rank], len(self.train_loader)))\n",
    "\n",
    "            # define neural nets model, criterion, and optimizer\n",
    "            self.model = util.select_model(10, args).cpu()\n",
    "            self.criterion = nn.CrossEntropyLoss().cpu()\n",
    "\n",
    "            # select optimizer according to algorithm\n",
    "            \n",
    "            selected_opt = self.algorithms[args.optimizer] #fednova #instantiating an\n",
    "            \n",
    "            self.optimizer = selected_opt(self.model.parameters(),\n",
    "                                    lr=args.lr,\n",
    "                                    gmf=args.gmf,\n",
    "                                    mu=args.mu,\n",
    "                                    ratio=self.DataRatios[rank],\n",
    "                                    momentum=args.momentum,\n",
    "                                    nesterov = False,\n",
    "                                    weight_decay=1e-4)\n",
    "\n",
    "        best_test_accuracy = 0\n",
    "\n",
    "        # Decide number of local updates per client\n",
    "        local_epochs = self.update_local_epochs(args.pattern, rank, round)\n",
    "        print('local_epochs: '+ str(local_epochs))\n",
    "\n",
    "        tau_i = local_epochs * len(self.train_loader)\n",
    "        logging.info(\"local epochs {} iterations {}\"\n",
    "                        .format(local_epochs, tau_i)) \n",
    "\n",
    "        # Decay learning rate according to round index\n",
    "        self.update_learning_rate(self.optimizer, round, args.lr)\n",
    "        # Clients locally train for several local epochs\n",
    "        for t in range(local_epochs):\n",
    "            self.train(self.model, self.criterion, self.optimizer, self.train_loader, t, rank)\n",
    "        \n",
    "    # # synchronize parameters\n",
    "    #     # dist.barrier()\n",
    "    #     comm_start = time.time()\n",
    "    #     optimizer.average()\n",
    "    #     # dist.barrier()\n",
    "    #     comm_end = time.time()\n",
    "    #     comm_time = comm_end - comm_start\n",
    "  \n",
    "    # evaluate test accuracy\n",
    "        test_acc = self.evaluate(self.model, self.test_loader)\n",
    "        if test_acc > best_test_accuracy:\n",
    "            best_test_accuracy = test_acc\n",
    "            \n",
    "    # record metrics\n",
    "        logging.info(\"Round {} test accuracy {:.3f}\".format(round, test_acc))\n",
    "        with open(args.out_fname, '+a') as f:\n",
    "            print('{ep},{itr},{filler},{filler},'\n",
    "                    '{filler},{filler},'\n",
    "                    '{val:.4f}'\n",
    "                    .format(ep=round, itr=-1,\n",
    "                            filler=-1, val=test_acc), file=f)\n",
    "\n",
    "        logging.info(\"Worker {} best test accuracy {:.3f}\"\n",
    "                     .format(rank, best_test_accuracy))\n",
    "        \n",
    "        return self.optimizer\n",
    "\n",
    "    def evaluate(self, model, test_loader) :\n",
    "        model.eval()\n",
    "        top1 = util.Meter(ptag='Acc')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data = data.cpu()\n",
    "                target = target.cpu()\n",
    "                outputs = model(data)\n",
    "                acc1 = util.comp_accuracy(outputs, target)\n",
    "                top1.update(acc1[0].item(), data.size(0))\n",
    "\n",
    "        return top1.avg\n",
    "\n",
    "        \n",
    "    def train(self, model, criterion, optimizer, loader, epoch, rank):\n",
    "        for name, param in model.named_parameters():\n",
    "            if \"classifier.6\" not in name:\n",
    "                param.requires_grad = False\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        losses = util.Meter(ptag='Loss')\n",
    "        top1 = util.Meter(ptag='Prec@1')\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(loader):\n",
    "            # data loading\n",
    "            data = data.cpu()\n",
    "            target = target.cpu()\n",
    "\n",
    "            # forward pass\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # gradient step\n",
    "            optimizer.step() #in fednova.py\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # write log files\n",
    "            train_acc = util.comp_accuracy(output, target)\n",
    "            \n",
    "\n",
    "            losses.update(loss.item(), data.size(0))\n",
    "            top1.update(train_acc[0].item(), data.size(0))\n",
    "\n",
    "            if batch_idx % args.print_freq == 0 and args.save:\n",
    "                logging.debug('epoch {} itr {}, '\n",
    "                            'rank {}, loss value {:.4f}, train accuracy {:.3f}'\n",
    "                            .format(epoch, batch_idx, rank, losses.avg, top1.avg))\n",
    "\n",
    "                with open(args.out_fname, '+a') as f:\n",
    "                    print('{ep},{itr},'\n",
    "                        '{loss.val:.4f},{loss.avg:.4f},'\n",
    "                        '{top1.val:.3f},{top1.avg:.3f},-1'\n",
    "                        .format(ep=epoch, itr=batch_idx,\n",
    "                                loss=losses, top1=top1), file=f)\n",
    "\n",
    "            with open(args.out_fname, '+a') as f:\n",
    "                print('{ep},{itr},'\n",
    "                    '{loss.val:.4f},{loss.avg:.4f},'\n",
    "                    '{top1.val:.3f},{top1.avg:.3f},-1'\n",
    "                    .format(ep=epoch, itr=batch_idx,\n",
    "                            loss=losses, top1=top1), file=f)\n",
    "\n",
    "\n",
    "    def update_local_epochs(self, pattern, rank: int, rnd: int):\n",
    "        if pattern == \"constant\":\n",
    "            return args.localE #basic case is what you input\n",
    "\n",
    "        if pattern == \"uniform_random\":\n",
    "            np.random.seed(2020+rank+rnd+args.seed)\n",
    "            return np.random.randint(low=2, high=args.localE, size=1)[0]\n",
    "\n",
    "\n",
    "    def update_learning_rate(self, optimizer, epoch, target_lr):\n",
    "        \"\"\"\n",
    "        1) Decay learning rate exponentially (epochs 30, 60, 80)\n",
    "        ** note: target_lr is the reference learning rate from which to scale down\n",
    "        \"\"\"\n",
    "        if epoch == int(args.rounds / 2):\n",
    "            lr = target_lr/10\n",
    "            logging.info('Updating learning rate to {}'.format(lr))\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "\n",
    "        if epoch == int(args.rounds * 0.75):\n",
    "            lr = target_lr/100\n",
    "            logging.info('Updating learning rate to {}'.format(lr))\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f3495cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#server level\n",
    "\n",
    "def average_opt(opts_list,weight=0, tau_eff=0):\n",
    "  \n",
    "    #Loop through list of optimizers and sum up the tau_eff to get a global tau_eff\n",
    "    for opt in opts_list:\n",
    "        \n",
    "    # tau_eff\n",
    "    \n",
    "        if weight == 0:\n",
    "            weight = opt.ratio\n",
    "        if tau_eff == 0: \n",
    "            tau_eff==0\n",
    "            if opt.mu != 0:\n",
    "\n",
    "                tau_eff_cuda = torch.tensor(opt.local_steps*opt.ratio).cpu()\n",
    "            else:\n",
    "                print(opt.local_normalizing_vec)\n",
    "                tau_eff_cuda = torch.tensor(opt.local_normalizing_vec*opt.ratio).cpu()\n",
    "            # dist.all_reduce(tau_eff_cuda, op=dist.ReduceOp.SUM)\n",
    "            tau_eff += tau_eff_cuda.item() #Returns the value of this tensor as a standard Python number. This only works for tensors with one element.\n",
    "\n",
    "    #tau_eff is saved, will be passed onward\n",
    "    #loop from the optimizers again to get param\n",
    "    \n",
    "    full_param_list = [] #contains updated params from all optimizers\n",
    "    \n",
    "    for opt in opts_list:\n",
    "    \n",
    "        param_list = []\n",
    "        \n",
    "        for group in opt.param_groups:\n",
    "            \n",
    "            for p in group['params']:\n",
    "                param_state = opt.state[p]\n",
    "                scale = tau_eff/opt.local_normalizing_vec\n",
    "                if 'cum_grad' in param_state: #pytorch automatically does this for you\n",
    "                    param_state['cum_grad'].mul_(weight*scale)\n",
    "                    param_list.append(param_state['cum_grad'])\n",
    "        \n",
    "        #this part isnt so important since we dont use momentum buffers at the moment\n",
    "        for group in opt.param_groups:\n",
    "            lr = group['lr']\n",
    "            for p in group['params']:\n",
    "                param_state = opt.state[p]\n",
    "                \n",
    "                #optional\n",
    "                if opt.gmf != 0:\n",
    "                    if 'global_momentum_buffer' not in param_state:\n",
    "                        buf = param_state['global_momentum_buffer'] = torch.clone(param_state['cum_grad']).detach()\n",
    "                        buf.div_(lr)\n",
    "                    else:\n",
    "                        buf = param_state['global_momentum_buffer']\n",
    "                        buf.mul_(opt.gmf).add_(1/lr, param_state['cum_grad'])\n",
    "                    if 'old_init' in param_state:\n",
    "                        param_state['old_init'].sub_(lr, buf)\n",
    "                else:\n",
    "                    if 'old_init' in param_state:\n",
    "\n",
    "                        param_state['old_init'].sub_(param_state['cum_grad'])\n",
    "                \n",
    "                if 'old_init' and 'cum_grad' in param_state:\n",
    "                    p.data.copy_(param_state['old_init'])\n",
    "                    param_state['cum_grad'].zero_()\n",
    "\n",
    "            # Reinitialize momentum buffer\n",
    "                if 'momentum_buffer' in param_state:\n",
    "                    param_state['momentum_buffer'].zero_()       \n",
    "                    \n",
    "        opt.local_counter = 0\n",
    "        opt.local_normalizing_vec = 0\n",
    "        opt.local_steps = 0\n",
    "        \n",
    "        full_param_list.append(param_list)\n",
    "        \n",
    "    return full_param_list\n",
    "\n",
    "def load_state(opt, updated_params):\n",
    "        \n",
    "    param_index = 0\n",
    "    \n",
    "    for group in opt.param_groups:\n",
    "        for p in group['params']:\n",
    "            param_state = opt.state[p]\n",
    "            if 'cum_grad' in param_state: #pytorch automatically does this for you\n",
    "                param_state['cum_grad']= updated_params[param_index] \n",
    "                param_index+=1  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ce0e6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUND 0\n",
      "==> load train data\n",
      "Files already downloaded and verified\n",
      "==> load test data\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - local epochs 1 iterations 79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_epochs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha)\n",
      "INFO - Round 0 test accuracy 10.000\n",
      "INFO - Worker 0 best test accuracy 10.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> load train data\n",
      "Files already downloaded and verified\n",
      "==> load test data\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - local epochs 1 iterations 79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_epochs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Round 0 test accuracy 10.000\n",
      "INFO - Worker 0 best test accuracy 10.000\n",
      "INFO - local epochs 1 iterations 79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "end\n",
      "ROUND 1\n",
      "local_epochs: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "client1 = client()\n",
    "client2 = client()\n",
    "\n",
    "num_rounds = 2\n",
    "\n",
    "opt1 = 0\n",
    "opt2 = 0\n",
    "\n",
    "init_run = True\n",
    "\n",
    "for r in range(num_rounds):\n",
    "        \n",
    "    opt1 = client1.run(init_run, rank=args.rank, size=1, round=r, opt=opt1)\n",
    "    opt2 = client2.run(init_run, rank=args.rank, size=1, round=r, opt=opt2)\n",
    "    \n",
    "    opt_list = [opt1, opt2]\n",
    "\n",
    "    updated_params = average_opt(opt_list)\n",
    "    \n",
    "    load_state(opt1, updated_params[0])\n",
    "    load_state(opt2, updated_params[1])\n",
    "    \n",
    "    print('end')\n",
    "    ## add a flag that will go to false when the first loop ends, so that it doesn't need to \n",
    "    init_run= False\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fa53ff979890fe2ee43ebe55261cf606546aca0d0d9789e51d7aa3559571d2be"
  },
  "kernelspec": {
   "display_name": "Python 3.5.6 64-bit ('venv-3.5.6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
